Here’s a crisp checklist table of the **discrete demos/examples** to ship in the v1 release. I grouped by track and tagged each with a priority so you can stage work.

| ID | Demo / Example                                     | Purpose (what it proves)                                          | How to run (high-level)                                                                                          | Inputs / Artifacts                                         | Deliverables                                             | Priority |
| -: | -------------------------------------------------- | ----------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------- | :------: |
| A1 | **Quickstart Screencast: Assistant Baseline Pass** | Capsules = policy+pedagogy+proof; bundle runs green end-to-end    | `pip install -r requirements.txt` → `python scripts/run_capsules.py --bundle bundles/assistant_baseline_v1.yaml` | `artifacts/examples/*`                                     | 5-min screencast, terminal log, README snippet           |    P0    |
| A2 | **Citation Gate: Fail → Fix → Pass**               | Witnesses enforce “cite it, or it didn’t happen”                  | Delete citations with `jq` → run single capsule → restore                                                        | `answer_with_citation.json` (+ edited variants)            | Before/after logs, 2 PNG screenshots                     |    P0    |
| A3 | **Tool JSON Contract: Fail → Fix → Pass**          | Schema-guarded tool calls block bad payloads                      | Break `.attendees` type, then repair → run `llm.tool_json_contract_v1`                                           | `tool_schema_book_meeting.json`, `tool_call_ok.json`       | Logs, tiny write-up for CI                               |    P0    |
| A4 | **PII Redaction Guard: Fail → Fix → Pass**         | PII hygiene is enforced by capsule                                | Inject email/phone → run `llm.pii_redaction_guard_v1` → redact → pass                                            | `pii_ok.json` (+ variants)                                 | Logs + redaction diff snippet                            |    P0    |
| A5 | **Problem-Solving 5-Step Witness**                 | Pedagogy + assertion: objective/assumptions/plan/evidence/summary | Run `pedagogy.problem_solving_v1` against OK and broken reports                                                  | `problem_solving_ok.json` (+ edited)                       | PASS/FAIL logs, rubric snippet                           |    P0    |
| B1 | **Cognitive Upgrade A/B #1 (Citations + PII)**     | Socratic + aphorisms improve structure & citations                | Run baseline prompt vs “Capsule Copypasta”; judge with fixed rubric                                              | N/A (use SPA or GPT UI)                                    | 2 outputs + judge table (scores)                         |    P0    |
| B2 | **Cognitive Upgrade A/B #2 (PR Risk & Impact)**    | Capsule drives risk tags, reversibility, test hints               | Baseline PR review vs capsule-inflated; judge on 5 criteria                                                      | Provided diff text                                         | 2 outputs + judge table                                  |    P0    |
| B3 | **Cognitive Upgrade A/B #3 (Ops Incident 5-Step)** | Capsule elicits backtest + reversible plan                        | Baseline ops plan vs capsule-inflated; judge                                                                     | Incident prompt text                                       | 2 outputs + judge table                                  |    P0    |
| C1 | **KG Export + SHACL Validate**                     | “KG-ready” is real: standards export passes shapes                | `python scripts/export_kg.py` → `pyshacl -s shacl/truthcapsule.shacl.ttl ...`                                    | `scripts/export_kg.py`, `shacl/*.ttl`                      | `artifacts/out/capsules.ttl` + validator PASS screenshot |    P0    |
| C2 | **Neo4j/Memgraph Loader + Queries**                | Property-graph integration and queryability                       | `:RUN scripts/load_neo4j.cypher` → run sample Cypher                                                             | `capsules.ndjson`, `load_neo4j.cypher`, `queries/*.cypher` | 3 query screenshots, sample results                      |    P1    |
| D1 | **Signing & Verification (Happy Path)**            | Core-content digest + ed25519 signature work                      | `python scripts/capsule_sign.py` → `capsule_verify.py`                                                           | one capsule, demo key (not committed)                      | Terminal logs, snippet for docs                          |    P0    |
| D2 | **Tamper Detection (Negative Path)**               | Signature breaks on content change                                | Modify `statement` → verify fails                                                                                | same as D1                                                 | FAIL log + explanation                                   |    P0    |
| D3 | **Sandboxed Witness Execution**                    | Security posture: hermetic runner                                 | Run witness via Docker profile: no net, RO FS, CPU/time caps                                                     | small Dockerfile or flag profile                           | Command + policy snippet + PASS log                      |    P1    |
| E1 | **CI Workflow: Lint + Gates + Compose**            | Capsules fit DevEx; green pipeline                                | Trigger `.github/workflows/*` on branch                                                                          | existing GH Actions                                        | CI run screenshots (checks green)                        |    P1    |
| E2 | **CI Failure Surfaces Root Cause**                 | Fast feedback shows failing gate & reason                         | Push PR with broken artifact → observe failing job                                                               | edited JSONs                                               | GH Checks screenshot with error excerpt                  |    P1    |
| F1 | **SPA Composer: Copy Capsule Prompt**              | One-click “cognitive upgrade” workflow                            | Use SPA to copy “Capsule Copypasta” into GPT; compare outputs                                                    | SPA `scripts/spa/*`                                        | 1-min silent capture GIF                                 |    P1    |
| F2 | **SPA: Judge Paste**                               | “One-paste judge” scores A vs B                                   | Paste judge prompt + outputs → score                                                                             | judge prompt                                               | Judge table screenshot                                   |    P1    |
| G1 | **Profiles Swap Demo (ci_llm vs ci_nonllm)**       | Profiles tailor the same capsules to different contexts           | Run with `profiles/ci_llm_judge.yaml` vs `ci_det.yaml`                                                           | `profiles/*.yaml`                                          | Two logs + tiny “diff in behavior” note                  |    P2    |
| G2 | **Bundle Composer CLI**                            | Bundles are first-class and reproducible                          | `compose_capsules_cli.py` to make a mini-bundle                                                                  | CLI + chosen IDs                                           | New bundle YAML + run log                                |    P2    |
| G3 | **Red-Team Assessment Capsule**                    | Red-teaming as a reusable artifact                                | Feed a known-bad answer → run `llm.red_team_assessment_v1`                                                       | crafted bad text                                           | FAIL log with flagged issues                             |    P2    |
| G4 | **Bias Checklist Capsule**                         | Bias hygiene gate                                                 | Run `llm.bias_checklist_v1` on opinionated text                                                                  | crafted biased text                                        | Findings log                                             |    P2    |
| G5 | **Counterfactual Probe + Plan Backtest**           | Reasoning robustness under counterfactuals                        | Run `llm.counterfactual_probe_v1` + `llm.plan_backtest_v1`                                                       | small scenario text                                        | Logs showing counterfactual & backtest sections          |    P2    |

**Notes:**

* **P0** = ship for v1 RC “Show & Tell”; **P1** = strong enhancers for HN/LinkedIn credibility; **P2** = nice-to-have/backup content.
* The three **A/B cognitive upgrade** demos (B1–B3) are your **proof** that Socratic + aphorisms materially improve model behavior-prioritize these alongside the CLI passes/fails.
* For **security**, do not commit private keys; demo signing with a locally generated key and include the policy snippet for sandboxing.
* For **KG**, commit the exported `capsules.ttl` and `capsules.ndjson` from a clean run so reviewers can open them instantly.