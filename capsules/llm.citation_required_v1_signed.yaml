id: llm.citation_required_v1_signed
version: 1.0.0
domain: llm
title: Citations required (signed demo)
statement: Answers must include at least one citation or abstain.
assumptions: []
pedagogy:
- kind: Socratic
  text: What source supports your key claim?
- kind: Aphorism
  text: Cite or abstain.
witnesses:
  - name: citations_cover_claims
    language: python
    env:
      ANSWER_PATH: artifacts/examples/answer_with_citation.json
      COVERAGE_MIN: "0.6"         # ≥60% declarative sentences must be cited
      DIVERSITY_MAX: "0.7"         # ≤70% of refs may come from one domain
      ALLOWLIST: "doi.org,arxiv.org,acm.org,ieee.org,nature.com,sciencemag.org,gov,edu"
      DOC_CLASS: ""                # set to "opinion" to SKIP
    code: |-
      import os, re, json, sys, pathlib
      from urllib.parse import urlparse

      ap = pathlib.Path(os.getenv("ANSWER_PATH","")).resolve()
      if not ap.exists():
          print(json.dumps({"witness":"citations_cover_claims","status":"FAIL","reason":"file-not-found","inputs":{"answer":str(ap)}}))
          sys.exit(1)

      # Opinion content can SKIP (policy-controlled)
      if os.getenv("DOC_CLASS","").lower() == "opinion":
          print(json.dumps({"witness":"citations_cover_claims","status":"SKIP","reason":"opinion-doc","inputs":{"answer":str(ap)}}))
          sys.exit(0)

      data = json.loads(ap.read_text(encoding="utf-8"))
      answer = (data.get("answer") or "").strip()
      refs = data.get("references") or []

      # Very light sentence segmentation (declarative-ish)
      sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', answer) if s.strip()]
      # Extract [n] citations
      cite_pat = re.compile(r'\[(\d+)\]')
      cited_flags = [bool(cite_pat.search(s)) for s in sentences]
      total = len(sentences)
      cited = sum(cited_flags)

      # Build ref index and domains
      ref_by_id = {int(r.get("id")): r for r in refs if isinstance(r.get("id"), int) and r.get("url")}
      domains = []
      for r in refs:
          try:
              host = urlparse(r.get("url","")).netloc.lower()
              if host: domains.append(host)
          except Exception:
              pass

      # Policies
      COVERAGE_MIN = float(os.getenv("COVERAGE_MIN","0.6"))
      DIVERSITY_MAX = float(os.getenv("DIVERSITY_MAX","0.7"))
      ALLOWLIST = [d.strip().lower() for d in os.getenv("ALLOWLIST","").split(",") if d.strip()]

      def domain_class(host):
          if host.endswith(".gov") or host.endswith(".edu") or host in ALLOWLIST or any(host.endswith(d) for d in ALLOWLIST):
              return "preferred"
          return "other"

      # Reference integrity checks
      # 1) No citations at all → FAIL (unless no declaratives → SKIP)
      if total == 0:
          print(json.dumps({"witness":"citations_cover_claims","status":"SKIP","reason":"no-declaratives","inputs":{"answer":str(ap)}}))
          sys.exit(0)

      found_cites = [int(n) for n in cite_pat.findall(answer)]
      if not found_cites:
          print(json.dumps({"witness":"citations_cover_claims","status":"FAIL","reason":"no-citations","inputs":{"answer":str(ap)}}))
          sys.exit(1)

      # 2) Every [n] must exist in references
      missing_map = sorted({n for n in set(found_cites) if n not in ref_by_id})
      if missing_map:
          print(json.dumps({"witness":"citations_cover_claims","status":"FAIL","reason":"missing-refs","missing_ids":missing_map,"inputs":{"answer":str(ap)}}))
          sys.exit(1)

      # 3) Sequential numbering starting at 1 (soft check)
      if ref_by_id and (min(ref_by_id) != 1 or max(ref_by_id) != len(ref_by_id)):
          # Not fatal, but record as warning in output payload
          numbering_warn = True
      else:
          numbering_warn = False

      # 4) Coverage threshold
      coverage = cited/total if total else 0.0
      if coverage < COVERAGE_MIN:
          print(json.dumps({"witness":"citations_cover_claims","status":"FAIL","reason":"coverage-too-low","coverage":coverage,"min":COVERAGE_MIN,"inputs":{"answer":str(ap)}}))
          sys.exit(1)

      # 5) Diversity (prevent single-domain echo chamber)
      from collections import Counter
      dom_counts = Counter(domains)
      if dom_counts:
          top_dom, top_cnt = dom_counts.most_common(1)[0]
          if top_cnt/len(domains) > DIVERSITY_MAX:
              print(json.dumps({"witness":"citations_cover_claims","status":"FAIL","reason":"source-too-concentrated","top_domain":top_dom,"share":top_cnt/len(domains),"max":DIVERSITY_MAX,"inputs":{"answer":str(ap)}}))
              sys.exit(1)

          # Disallow all-wikipedia (or single non-preferred domain) cases
          if len(dom_counts) == 1 and ("wikipedia.org" in dom_counts):
              print(json.dumps({"witness":"citations_cover_claims","status":"FAIL","reason":"all-wikipedia","inputs":{"answer":str(ap)}}))
              sys.exit(1)

      # 6) Allowlist presence (at least one preferred or academic/governmental source)
      has_pref = any(domain_class(h)=="preferred" for h in domains)
      if not has_pref and refs:
          print(json.dumps({"witness":"citations_cover_claims","status":"FAIL","reason":"no-preferred-sources","inputs":{"answer":str(ap)}}))
          sys.exit(1)

      out = {
          "witness":"citations_cover_claims",
          "status":"PASS",
          "coverage": coverage,
          "sequential_numbering_warn": numbering_warn,
          "totals":{"sentences": total, "cited": cited, "refs": len(refs)},
          "inputs":{"answer": str(ap)}
      }
      print(json.dumps(out, indent=2))
      sys.exit(0)
provenance:
  author: Truth Capsules Demo
  license: MIT
  schema: provenance.v1
  org: Truth Capsules
  created: '2025-11-07T04:51:20.658919Z'
  signing:
    algorithm: ed25519
    public_key_pem: '-----BEGIN PUBLIC KEY-----

      MCowBQYDK2VwAyEAdkKe+ZblRkk/8oGtvid7tisLS8L3j12LjGHLC05vnDs=

      -----END PUBLIC KEY-----

      '
    digest: b7862aabe696451c37f4e870ece9ae18b37344ef14cf48561dbadf40a5bd5d42
    signature_b64: VJ8i2ZLMIER3ITahlPQZ435w/ZxRAwyGRnPqboVQG78+GUe2alcBd5BS/1KzjtzMxi+B32TvaSbu7tYf3rNSBg==
    status: approved
